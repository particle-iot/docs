{
  "type": "libraries",
  "id": "flashee-eeprom",
  "links": {
    "download": "https://api.particle.io/v1/libraries/flashee-eeprom/archive/0.1.8.tar.gz"
  },
  "attributes": {
    "name": "flashee-eeprom",
    "version": "0.1.8",
    "installs": 9299,
    "license": "GPLv3",
    "author": "mdma <mat dot mcgowan+spark at gmail dot com>",
    "sentence": "Provides EEPROM-like access to the external flash on the spark. The library takes care of performing page erases and emulates the byte-level erase feature of eeprom.",
    "url": "https://github.com/m-mcgowan/spark-flashee-eeprom",
    "repository": "https://github.com/m-mcgowan/spark-flashee-eeprom.git",
    "architectures": [],
    "visibility": "public",
    "mine": false
  },
  "kind": "community library",
  "letter": "f",
  "cardUrl": "/cards/libraries/f/flashee-eeprom",
  "versions": {
    "0.1.8": {
      "builds": {
        "2.0.1": {
          "photon": {
            "filemanager": true,
            "integration-tests": true,
            "performace-profile": true
          },
          "electron": {
            "filemanager": true,
            "integration-tests": true,
            "performace-profile": true
          },
          "argon": {
            "filemanager": false,
            "integration-tests": false,
            "performace-profile": false
          },
          "boron": {
            "filemanager": false,
            "integration-tests": false,
            "performace-profile": false
          }
        },
        "1.5.2": {
          "photon": {
            "filemanager": true,
            "integration-tests": true,
            "performace-profile": true
          },
          "electron": {
            "filemanager": true,
            "integration-tests": true,
            "performace-profile": true
          },
          "argon": {
            "filemanager": false,
            "integration-tests": false,
            "performace-profile": false
          },
          "boron": {
            "filemanager": false,
            "integration-tests": false,
            "performace-profile": false
          }
        }
      }
    }
  },
  "readme": "flashee\n=======\n\nEeprom emulation using external flash on the [Spark Core](http://spark.io).\nIncludes unit tests cross compiled to regular gcc, and on-device integration tests.\n\nElevator Pitch\n--------------\n\nThis library allows client code to write to the external flash as if it were\nEEPROM. It takes care of performing erases when required to ensure\ndata is written correctly. 3 storage schemes are provided that offer\ndifferent trade-offs in storage efficiency for increased number of write\ncycles over the lifetime of the device.\n\nKey features:\n\n- Provides persistent storage using the external flash on the Soark Core, and Particle P1, and emulated EEPROM on the Photon.\n- EEPROM-like access - (byte erasable) no need to worry about erasing pages to ensure data integrity.\n- 3 different types of eeprom emulations providing speed/erase cycle tradeoff.\n- Wear leveling and page allocation on demand for increased endurance\n- Circular buffers for logs, temporary data etc.\n- Stream access to the storage for convenient read and write of multiple values.\n- File System support: a FAT filesystem can be stored in a region of flash.\n\nGetting Started\n===============\n\nSetting up your environment\n---------------------------\n\n- If you're using the online IDE, click on \"libraries\", then \"flashee-eeprom\", then \"Include in app\", and finally select the app you\nwant to use the library with. This will automatically add the include directive to bring in the library header file.\n\n- If you're compiling locally using the core-firmware makefile, clone the github repo to the same folder that contains\n`core-firmware` (so core-firmware and spark-flashee-eeprom in the same folder on your machine.).\n - edit `core-firmware\\src\\build.mk` and add these additional lines:\n\n ```\n    INCLUDE_DIRS += ../spark-flashee-eeprom/firmware\n    CPPSRC += ../spark-flashee-eeprom/firmware/flashee-eeprom.cpp\n    CPPSRC += ../spark-flashee-eeprom/firmware/ff.cpp\n ```\n\nUsing the library\n-----------------\n\nTo use the library in application code, include the header file and import the namespace, like this:\n\nIn the online IDE:\n```c++\n    #include \"flashee-eeprom/flashee-eeprom.h\"\n    using namespace Flashee;\n```\n\nLocal build:\n```c++\n    #include \"flashee-eeprom.h\"\n    using namespace Flashee;\n```\n\nTo gain access to the services of `flashee`, you use the `Devices` class, which provides methods for creating the\nvarious flash devices available.  You typically call a `Devices` method in `setup()` and store the result in a\nglobal pointer. The method `createDefaultStore()` provides EEPROM-like access to\nthe persistent storage on the device.\n\n```c++\n    FlashDevice* flash;\n\n    void setup() {\n        flash = Devices::createDefaultStore();\n    }\n```\n\nData is read and written like this:\n\n```\n    int value;\n    flash->read(value, 10); // read value from address 10\n    value += 20;\n    flash->write(value, 10);    // increment and write back\n```\n\nAny kind of data can be written. For writing strings, use `writeString`\n\n```\n    flash->writeString(\"I am Spartacus\", 10);\n```\n\nParticle Photon\n---------------\n\nThe persistent storage provided by flashee uses the 2Kbytes of emulated EEPROM already\npresent in the Particle firmware. The main purpose of this is to make it easy to\nport code between platofrms, in cases where code needs to be proted to the Photon,\nand the small size of the persistent memory isn't an issue for the target application.\n\n\nSpark Core / Particle P1\n------------------------\n\nThe Spark Core has 1.5MByte of usable external flash memory. The Particle P1 has 1MByte.\n\nOn these systems, the library offers more control over the type of EEPROM-emulation\nused. On these devices.  The memory created by calling `createDefaultStore()` is the same\nas\n\n```\n    FlashDevice* flash = Devices::createAddressErase();\n```\n\nThis reserves the first 1MByte in the external flash for a memory device that provides byte-level erases. This offers\nthe best erase cycle count at the cost of an 8x overhead in storage. This scheme gives a maximum of 128Kb of storage.\n\nIf you need more than 128Kb of rewritable storage (available on the Spark Core), the next step down the endurance ladder is the wear leveling scheme:\n\n```c++\n    FlashDevice* flash = Devices::createWearLevelErase();\n```\n\nThis uses wear leveling to spread the erases out over the flash region. Endurance an order of magnitude less than the address erase scheme\nabove, but is potentially 1-2 orders of magnitude better than manually erasing and writing directly to flash.\nThis scheme has much less overhead, and can offer up to 1MB of storage.\n\n\nThe key difference between flash and eeprom is that with flash memory you cannot normally erase a single byte, but have to\nerase a whole page. This library takes care of that, and presents an interface to the flash device that makes it\nbehave like eeprom, which does support rewrites of data without having to perform a page erase.\n\n\nCircular Buffers\n================\n\nThe library provides a [circular buffer](http://en.wikipedia.org/wiki/Circular_buffer) implementation that\nallows data to be written to a storage device and subsequently read out again.\n\nTo create a circular buffer, specify the start and end address (as usual, these should be on page boundaries):\n\n```c++\n    CircularBuffer* buffer = Devices::createCircularBuffer(4096*256, 4096*384);\n```\n\nThis will create a circular buffer from page 256 through to page 384 (0.5MB).\n\nWriting to the buffer:\n```c++\n    MyStruct data = ...;\n    bool success = buffer->write(&data, sizeof(data));\n```\n\nIf there is not room in the buffer for the complete block, the call fails and returns `false`.\n\nReading from the buffer:\n\n```c++\n    MyStruct data = ...;\n    bool success = buffer->read(&data, sizeof(data));\n```\nData is read from the buffer in the same sequence it was written.\n\n\nTo determine how much data can be read from or written to the buffer:\n\n```c++\n    page_size_t can_read = buffer->available();\n    page_size_t maximum = buffer->capacity();\n    page_size_t can_store = buffer->free();\n```\n\nNote that the buffer is just a large block of bytes. It is up to the caller to be sure that the way the data is\nretrieved is compatible with how it was stored.\n\nNormally, the buffer is all or nothing - if the requested number of bytes cannot be read or written, then no bytes\nare read or written. There are 'soft' variants of the read/write methods that allow less than the specified number of\nbytes to be read/written.\n\nNote that although the circular buffer storage is in flash, the data is essentially non-persistent. On reset, a new\nbuffer is created which ignores the data in flash.\n\n\nStreaming\n=========\nThe streaming classes provide a higher-level access to the storage. For example:\n\n```c++\n    FlashDevice* device = Devices::createWearLevelErase();\n    FlashWriter writer(device);\n    writer.writeString(\"Hello World\");\n    writer.write(42);\n```\n\nAnd this data can then be read back later:\n\n```c++\n    FlashDevice* device = Devices::createWearLevelErase();\n    FlashReader reader(device);\n    char buf[50];\n    reader.readString(buf);\n    int answer = reader.readInt();\n```\n\nFile System\n===========\nFlashee includes support for storing a FAT filesystem in an area of flash. The FAT support is provided by\n[fatfs](http://elm-chan.org/fsw/ff/00index_e.html) library.\n\nTo set side a region of flash for file storage, use the `createFATRegion()` function:\n\n```c++\n   FATFS fs;\n   FRESULT result = Devices::createFATRegion(0, 4096*384, &fs);\n   if (result==FR_OK) {\n        // FS ok - use fatfs functions to read/write to the filesystem\n   }\n```\n\nThis allocates area of flash for a FAT filesystem. The default behaviour is to format the region,, if it is not recognized\nas a valid FAT filesystem. This can be controlled via the 4th parameter to the function.\n\nSince it's based on Flashee's eraseable storage, the filesystem is fully rewritable. See the FileXXX.cpp examples\nfor more details on using fatfs with flashee.\n\n\nCoding tips\n===========\n\n* The [main API](firmware/flashee-eeprom.h) to the library is provided by the `Devices` class, which is a factory for obtaining various flash-access\ndevices, and the `FlashDevice` abstract base class, which defines the operations of a flash device.\n\n* The spark external flash has 384 pages, each 4096 bytes in size and is managed as the device provided via the `Devices::userFlash()` method.\nRather than hard-code these page count and page size constants, you can make the code more flexible for future changes by using `Devices::userFlash().pageSize()` and\n `Devices::userFlash().pageCount()`.\n\n* when writing data to a device, try to write in as few blocks as possible (particularly if the data is overwriting previously eritten data).\n This will reduce the number of erases performed by the library, particularly for the wear leveling scheme. When using\n the Address Erase strategy, then byte by byte writes are fine.\n\n* At present, the maximum contiguous area that the Wear Leveling and Address Erase\n  schemes can occupy is 1MB (256 pages). This is to keep runtime memory overhead to a minimum. This restriction may later be relaxed. For now,\n  A workaround for accessing more than 1MB is to create more than one device, specifying non-overlapping regions for each device.\n  This 1MB limitation is not present for circular buffers, nor for the Single Page Wear scheme.\n\n* It's possible to create several different devices and have them all active at once, so long as they are in separate regions. For example\n\n```c++\n    FlashDevice* eeprom = Devices::createAddressErase(0, 256*4096);\n    CircularBuffer* logBuffer = Devices::createCircularBuffer(256*4096, 384*4096);\n```\n\nThis creates a byte erasable eeprom device in the first 1MB, and a circular buffer in the final 0.5MB.\n\n\nTesting\n=======\nThis Flashee-Eeprom library contains unit tests and integration tests.\nUnit tests are compiled using GCC and run on the host platform.\nIntegration tests are executed on the embedded device.\n\nUnit Tests\n----------\nThe unit tests exercise the types of flash eeprom emulation, as well as stressing individual classes and methods.\nThe tests run against mock or fake implementations of a flash device.\n\nTo run the unit tests:\n\n    1. clone this repo to your desktop machine\n    2. (if running Windows, be sure to install MinGW installed also.)\n    3. cd to flashee-eeprom/firmware/test\n    4. run `make test`\n\nAfter the tests are built, they are automatically run. You should then see output similar to this:\n\n    [----------] 4 tests from FlashDeviceRegionTest\n    [ RUN      ] FlashDeviceRegionTest.CanCreateSubregion\n    [       OK ] FlashDeviceRegionTest.CanCreateSubregion (0 ms)\n    [ RUN      ] FlashDeviceRegionTest.SubregionOutOfRangeFails\n    [       OK ] FlashDeviceRegionTest.SubregionOutOfRangeFails (0 ms)\n    [ RUN      ] FlashDeviceRegionTest.NestedSubregionOutOfRangeFails\n    [       OK ] FlashDeviceRegionTest.NestedSubregionOutOfRangeFails (0 ms)\n    [ RUN      ] FlashDeviceRegionTest.NestedSubregionInRangeOk\n    [       OK ] FlashDeviceRegionTest.NestedSubregionInRangeOk (1 ms)\n    [----------] 4 tests from FlashDeviceRegionTest (4 ms total)\n\n    [----------] 2 tests from FakeFlashDeviceTest\n    [ RUN      ] FakeFlashDeviceTest.CorrectSize\n    [       OK ] FakeFlashDeviceTest.CorrectSize (0 ms)\n    [ RUN      ] FakeFlashDeviceTest.CorrectSize2\n    [       OK ] FakeFlashDeviceTest.CorrectSize2 (0 ms)\n    [----------] 2 tests from FakeFlashDeviceTest (1 ms total)\n\n    [----------] Global test environment tear-down\n    [==========] 102 tests from 8 test cases ran. (1714 ms total)\n    [  PASSED  ] 102 tests.\n\n\nIntegration Tests\n-----------------\nThe integration tests combine the real external flash device in the spark core with the eeprom emulation layers.\nThe aim of the integration test is to test that the library functions as a whole.\n\nThe integration test is available from the online IDE, under `examples/integration-test.cpp`.\n\nTo run the integration test:\n\n    1. build and flash integration-test-cpp to the core\n    2. use a serial monitor to connect to the core locally via the USB serial interface\n    3. press 't' to start the tests.\n\nIf all goes well, you should see output in the serial monitor after a few seconds. The whole suite takes about a minute\nto run and produces output similar to this:\n\n    Running tests\n    Test CanWriteAfterErase passed.\n    Test ErasePageNoEffectOnOtherPages passed.\n    Test ErasePageResetsData passed.\n    Test EraseWriteAllowsBitsToBeSet passed.\n    Test EraseWritePreservesRestOfPage passed.\n    Test HasNonZeroPageCount passed.\n    Test HasNonZeroPageSize passed.\n    Test PageAddress passed.\n    Test RepeatedEraseWritePreservesRestOfPage passed.\n    Test SparkFlashCanWriteEvenAddressBytes passed.\n    Test SparkFlashCanWriteEvenAddressBytesOddLength passed.\n    Test SparkFlashCanWriteOddBytes passed.\n    Test SuccessiveWrittenValuesAreAnded passed.\n    Test WriteDistinctValueToPages passed.\n    Test summary: 15 passed, 0 failed, and 0 skipped, out of 15 test(s).\n\nPerformance Profiling\n=====================\nThe example code [`performance-profile.cpp`](firmware/examples/performace-profile.cpp) profiles the direct flash\naccess and 2 types of eeprom emulation. The results are below:\n\n```\nRunning tests\nPerformance test: Address level erase\nBuffer size: 128, total bytes: 16352\n Erase: throughput 1090 Kbytes/sec\n Verify erase: throughput 50 Kbytes/sec\n Write: throughput 11 Kbytes/sec\n Verify write: throughput 49 Kbytes/sec\n Rewrite: throughput 11 Kbytes/sec\n Verify rewrite: throughput 49 Kbytes/sec\n\nBuffer size: 512, total bytes: 16352\n Erase: throughput 1168 Kbytes/sec\n Verify erase: throughput 50 Kbytes/sec\n Write: throughput 11 Kbytes/sec\n Verify write: throughput 50 Kbytes/sec\n Rewrite: throughput 11 Kbytes/sec\n Verify rewrite: throughput 49 Kbytes/sec\n\nBuffer size: 2048, total bytes: 16352\n Erase: throughput 1168 Kbytes/sec\n Verify erase: throughput 50 Kbytes/sec\n Write: throughput 11 Kbytes/sec\n Verify write: throughput 50 Kbytes/sec\n Rewrite: throughput 11 Kbytes/sec\n Verify rewrite: throughput 49 Kbytes/sec\n\n\nPerformance test: Wear level page erase\nBuffer size: 128, total bytes: 131008\n Erase: throughput 9357 Kbytes/sec\n Verify erase: throughput 385 Kbytes/sec\n Write: throughput 120 Kbytes/sec\n Verify write: throughput 385 Kbytes/sec\n Rewrite: throughput 45 Kbytes/sec\n Verify rewrite: throughput 385 Kbytes/sec\n\nBuffer size: 512, total bytes: 131008\n Erase: throughput 9357 Kbytes/sec\n Verify erase: throughput 404 Kbytes/sec\n Write: throughput 122 Kbytes/sec\n Verify write: throughput 404 Kbytes/sec\n Rewrite: throughput 72 Kbytes/sec\n Verify rewrite: throughput 404 Kbytes/sec\n\nBuffer size: 2048, total bytes: 131008\n Erase: throughput 8733 Kbytes/sec\n Verify erase: throughput 409 Kbytes/sec\n Write: throughput 123 Kbytes/sec\n Verify write: throughput 409 Kbytes/sec\n Rewrite: throughput 86 Kbytes/sec\n Verify rewrite: throughput 410 Kbytes/sec\n\n\nPerformance test: Basic flash access\nBuffer size: 128, total bytes: 262144\n Erase: throughput 292 Kbytes/sec\n Verify erase: throughput 393 Kbytes/sec\n Write: throughput 121 Kbytes/sec\n Verify write: throughput 393 Kbytes/sec\n Rewrite: N/A (not supported or failed.)\n Verify rewrite: N/A (not supported or failed.)\n\nBuffer size: 512, total bytes: 262144\n Erase: throughput 292 Kbytes/sec\n Verify erase: throughput 406 Kbytes/sec\n Write: throughput 122 Kbytes/sec\n Verify write: throughput 407 Kbytes/sec\n Rewrite: N/A (not supported or failed.)\n Verify rewrite: N/A (not supported or failed.)\n\nBuffer size: 2048, total bytes: 262144\n Erase: throughput 292 Kbytes/sec\n Verify erase: throughput 410 Kbytes/sec\n Write: throughput 123 Kbytes/sec\n Verify write: throughput 410 Kbytes/sec\n Rewrite: N/A (not supported or failed.)\n Verify rewrite: N/A (not supported or failed.)\n\n\nTest complete.\n\n```\n\nThe Single Wear Page scheme was not tested since this would incur a high number of erases to a single page, possibly\nprematurely destroying that page.\n\nConclusions from the data above:\n\n * the performace of page level erase is on par with direct flash access - 404 KiB/s read, and 123 KiB/s write. Larger\n   buffers provide a slight performance increase.\n * address level erase has the slowest performace, at 11KiB/s write and 50KiB/s read, but this has an order of magnitude\n   better erase performance.\n\n level erase also has an order of magnitude better endurance since it erases pages less often.)\n\n * writing direct to flash is about 7x faster (including erase time), but doesn't support rewrites.\n    (That's the main contribution of this library.)\n\n * compared to an Arduino, all schemes are significantly faster.\n    The arduino eeprom requires 3.3ms to erase a single byte - equivalent to 0.3 Kb/s throughput.\n\n * with the wear leveling scheme, writing larger buffers improves throughput since the number of page erases and copies required is reduced.\n\nInternally, the eeprom emulation uses a 128 byte buffer. This explains why the address erase scheme shows no performance\nimprovement when called with larger buffers. Increasing the size of the internal buffer may improve performance - see\n`STACK_BUFFER_SIZE` in the headers.\n\nFinally, it's wise not to pick a scheme based on performance, unless performance is the defining characteristic of\nyour application, in which case it is best to write direct to flash and manage page erases by hand.\n\n\nImplementation Details\n======================\n\nDevelopment\n-----------\nI developed the library initially as a standalone library compiled on regular\ngnu c++ on my desktop. Compared to embedded development, this allowed a\nfaster development cycle and easier debugging. For testing, the flash\nmemory was faked using a `FakeFlashDevice` class that emulated a flash\ndevice in memory (ANDed writes, page erases and read/write only on\neven address/even length.)\n\n\nEmulation Layers\n----------------\n\nThe library provides a random read/write access device, similar to\nEEPROM from the external flash on the spark. It's implemented as one or more layers on\ntop of the flash API. These layers take care of page erases and provide multiple\nwrites to the same logical address. It does this through two key techniques:\n\n * redundant storage - multiple writes to the same logical address are\nimplemented as writes to distinct physical addresses in flash.\n\n * wear leveling: a mapping from logical pages to physical pages is\nmaintained. When a logical page is erased, it is assigned to a new free\nphysical page (using an even random distribution). This ensures updates\nto a single page are spread out over the area of flash allocated, so the\nwear is spread out across many pages rather than just one.\n\n\nFor EEPROM-like storage, there are several implementations that\nprovide the same API (so they can be used interchangably), with each\nimplementation providing a different trade-off between storage efficiency\nand the maximum erase wear for any given page.\n\n### Direct flash\n\nThis is simply direct access to the flash memory. Automatic erase before\nwrite is not supported. On construction, the range of pages in flash to be\nused is specified. This doesn't provide EEPROM semantics, but rather\nprovide the base storage for the implementations that follow.\n\nThe following implementations provide EEPROM-like semantics,\nsupporting free read/write to addresses. Despite the complexity of some\nof the schemes, the client simply sees a linear address range that they\ncan read and write to, and doesn't need to concernt themselves so much\nwith the internal workings. To provide EEPROM-like semantics, the\nimplementations take care of performing erases when they are\nnecessary. This is typically when a destructive write is performed - writing\na 1 bit to a location where there was previously a 0.\n\nDirect with erase copy\n......................\n\nThis scheme stores the data directly in flash at the address\nspecified. When a page erase is needed to reset 0 bits back to 1, the page\nis copied to a reserved page in flash. The original page is then erased\nand the data copied back.\n\nThis makes the most efficient use of space in terms of storage used, but\nat the cost of considerable wear on the reserved page, which is erased for every\nsingle erasure on any page, since every write that requires an erase will erase\nthe reserved page.\n\nThis implementation is recommended for cases when the data changes\nless than 10^5 times during the lifetime of the system (the maximum wear\nfor a page in external flash being 10^5.)\n\n### Wear Levelled storage\n\nThis uses a specified region of flash where logical pages are mapped to\ntheir actual page location in flash. This allows the actual location of a logical\naddress to be changed, such as when erasing a page. When a logical page is\nerased, it is assigned to a new physical page in flash. This distributes the\nwear over all free and changing pages. This happens transparently - to\nthe user they simply see the same API - `FlashDevice`.\n\nThis scheme reserves 2 bytes per page for housekeeping. There is also 1 page used for\nhousekeeping, at at a minimum there must be at least 1 free page. Initially all pages are free\nuntil data is written to the device. When a page is erased, it is mapped to a different\nphysical page, reducing the wear per page.\n\nFor example, if you have reserved region in flash, and usage is such that\na single page takes most of the changes, then allocating 10 free pages\nwill ensure those changes are spread out over 10+1 pages (the erased\npage itself is freed so becomes part of the free pool.). The number of\nerases for any given page can then as high as 10^6.  The EEPROM in\nthe arduino allows 10^5 erases, this is already surpassing eeprom wear\nlevels.\n\nRedundant Storage\n.................\n\nThis scheme allows multiple destructive writes to the same logical\naddress without requiring an erase in the underlying flash. It achieves this by\nrepresenting each logical byte as a 8-byte slot. Data is written until the slot\nis full, and then an erase is required. This scheme supports 7 destructive\nwrites before the page is erased. Note that when a page is erased, all\naddresses are reset back to the 7 write capacity.\n\nThis is naturally the least efficient in terms of storage, but also offers the\nleast wear, reducing the number of erases required by approximately an\norder of magnitude. When combined with the wear levelled storage,\ndestructive writes in the order of 10^8 can be achieved over the lifetime\nof the device.\n\n### Combining Layers\n\nAll the implementations of the eeprom emulation expose the same interface, and the higher level\nstorage schemes (Wear Levelled storage/Redundant storage) also\nexpect an implementation of that interface as their base storage. This\nallows the implementations to be stacked, effectively combining them.\n\nThere two utility devices: `PageSpanFlashDevice` and `FlashDeviceRegion`\nthat make working with devices easier. `PageSpanFlashDevice` allows the\nvarious read and write methods to accept a buffer that is larger than a\npage or allow reads and writes to span page boundaries.\nFlashDeviceRegion provides a flash device that maps to a contiguous\nsubregion in some other flash device. This is used to partition a given\ndevice into distinct areas.\n\nThese utility devices are coded to the FlashDevice interface.\nConsequently, they can be used both with the physical flash device, as\nwell as any of the higher level layers. Typically, the PageSpanFlashDevice\nis placed on the top-level device that the client uses, so the client is free\nto read arbitrary blocks. The FlashDeviceRegion is used to divide up the\nphysical flash memory into distinct areas.\n\n",
  "allVersions": [
    "0.1.8",
    "0.1.7",
    "0.1.6",
    "0.1.5",
    "0.1.4",
    "0.1.3",
    "0.1.2",
    "0.1.1",
    "0.1.0"
  ]
}